{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seq2seq_base\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 100\n",
    "SNIPPET_LEN = 40\n",
    "\n",
    "save_features = \"/storage/egor/tmp/feat_token2ind.pkl\"\n",
    "with open(save_features, \"rb\") as f:\n",
    "    feat, token2ind = pickle.load(f)\n",
    "\n",
    "# add start \"symbol\"\n",
    "START = token2ind.setdefault(\"starting_symbol_for_decoding_seq\", len(token2ind))\n",
    "\n",
    "# increment all values in dictionary by 1\n",
    "token2ind_new = dict((k, v + 1) for k, v in token2ind.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_samples(raw_feat, enc_len=MAX_LEN, dec_len=SNIPPET_LEN, max_n_samples=10 ** 5,\n",
    "                    min_seq_len=50):\n",
    "    enc = []\n",
    "    dec = []\n",
    "    \n",
    "    while len(enc) < max_n_samples:\n",
    "        # select random sequence of tokens\n",
    "        ind = randint(len(raw_feat))\n",
    "        seq = raw_feat[ind]\n",
    "        \n",
    "        if len(seq) < min_seq_len:\n",
    "            continue\n",
    "        \n",
    "        ind = randint(len(seq))\n",
    "        start_ind = max(0, ind - enc_len)\n",
    "        # increase all values by one to make 0 padding\n",
    "        enc.append(list(map(lambda x: x + 1, seq[start_ind:ind])))\n",
    "        end_ind = min(len(seq), ind + dec_len)\n",
    "        # prepend start symbol\n",
    "        dec.append(list(map(lambda x: x + 1, [START] + seq[ind:end_ind])))\n",
    "    dec = pad_sequences(dec, maxlen=dec_len)\n",
    "    dec_in = dec[:, :-1]\n",
    "    dec_target = dec[:, 1:]\n",
    "    enc = pad_sequences(enc, maxlen=enc_len, padding=\"post\")\n",
    "    dec_in[:, 0] = enc[:, -1]\n",
    "    return pad_sequences(enc, maxlen=enc_len), dec_in, dec_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = feat[:-2000]\n",
    "val = feat[-2000:]\n",
    "\n",
    "train_enc, train_dec_in, train_dec_target = prepare_samples(train)\n",
    "val_enc, val_dec_in, val_dec_target = prepare_samples(val, max_n_samples=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_token2ind = token2ind_new\n",
    "enc_latent_dim = 256\n",
    "optimizer = \"rmsprop\"\n",
    "encoder_seq_len = MAX_LEN\n",
    "decoder_seq_len = SNIPPET_LEN\n",
    "\n",
    "s2s = seq2seq_base.Seq2SeqBase(enc_token2ind=enc_token2ind, enc_latent_dim=enc_latent_dim, optimizer=optimizer,\n",
    "                               encoder_seq_len=encoder_seq_len, decoder_seq_len=decoder_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 1\n",
    "for _ in range(3):\n",
    "    train_enc, train_dec_in, train_dec_target = prepare_samples(train)\n",
    "    val_enc, val_dec_in, val_dec_target = prepare_samples(val, max_n_samples=20000)\n",
    "    s2s.train_model.fit([train_enc, train_dec_in], np.expand_dims(train_dec_target, axis=-1),\n",
    "                        batch_size=batch_size, shuffle=True, epochs=epochs,\n",
    "                        validation_data=[[val_enc, val_dec_in], np.expand_dims(val_dec_target, axis=-1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "val_pred = s2s.train_model.predict([val_enc, val_dec_in])\n",
    "val_pred_ind = val_pred.argmax(axis=-1)\n",
    "\n",
    "gt = np.hstack([val_dec_target[:, i] for i in range(39)])\n",
    "pred = np.hstack([val_pred_ind[:, i] for i in range(39)])\n",
    "gt.shape, pred.shape\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(pred, gt)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "class_names = [x[1] for x in list(sorted(s2s.ind2token.items(), key=lambda x: x[0]))]\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inference step for several samples\n",
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training test)\n",
    "    # for trying out decoding.\n",
    "    input_seq = val_enc[seq_index: seq_index + 1]\n",
    "    input_sent = \" \".join([s2s.ind2token.get(ind, \"\")\n",
    "                          for ind in input_seq[0]]).strip()\n",
    "    expected_output = \" \".join(s2s.ind2token.get(ind, \"\") \n",
    "                               for ind in val_dec_target[seq_index: seq_index + 1][0])\n",
    "    decoded_sentence = s2s.decode_sequence(input_seq, s2s.ind2token.get(input_seq[0][-1], \"\"))\n",
    "    print('Input sentence:', input_sent)\n",
    "    print('Decoded sentence:', decoded_sentence)\n",
    "    print(\"-\" * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}